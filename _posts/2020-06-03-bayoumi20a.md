---
title: Tighter Theory for Local SGD on Identical and Heterogeneous Data
abstract: 'We provide a new analysis of local SGD, removing unnecessary assumptions
  and elaborating on the difference between two data regimes: identical and heterogeneous.
  In both cases, we improve the existing theory and provide values of the optimal
  stepsize and optimal number of local iterations. Our bounds are based on a new notion
  of variance that is specific to local SGD methods with different data. The tightness
  of our results is guaranteed by recovering known statements when we plug $H=1$,
  where $H$ is the number of local steps. The empirical evidence further validates
  the severe impact of data  heterogeneity on the performance of local SGD.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bayoumi20a
month: 0
tex_title: Tighter Theory for Local SGD on Identical and Heterogeneous Data
firstpage: 4519
lastpage: 4529
page: 4519-4529
order: 4519
cycles: false
bibtex_author: Bayoumi, Ahmed Khaled Ragab and Mishchenko, Konstantin and Richtarik,
  Peter
author:
- given: Ahmed 
  family: Khaled
- given: Konstantin
  family: Mishchenko
- given: Peter
  family: Richtarik
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/bayoumi20a/bayoumi20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/bayoumi20a/bayoumi20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
